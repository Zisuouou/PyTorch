{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3f18e6",
   "metadata": {},
   "source": [
    "### 1. 필요 모듈 설치 (torch, torchvision, torchaudio, transformers, pytorch-lightning)\n",
    "### 2. 데이터 셋 준비\n",
    "    - roboflow 에서 데이터셋 구현 후 coco format 으로 다운로드\n",
    "### 3. Dataset : VisDrone Dataset(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd13f7a",
   "metadata": {},
   "source": [
    "**Pytorch Custom dataset : CustomDataset 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf531eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     # 생성자, 데이터를 전처리 하는 부분\n",
    "#     def __init__(self):\n",
    "#     # 데이터셋의 총 길이를 반환하는 부분\n",
    "#     def __len__(self):\n",
    "#     # idx(인덱스)에 해당하는 입출력 데이터를 반환\n",
    "#     def __getitem__(self, idx):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d69503",
   "metadata": {},
   "source": [
    "- COCO-Format 이라서 CustomDataset 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0eeccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, processor, train=True):\n",
    "        ann_file = os.path.join(img_folder, \"_annotations.coco.json\" if train else \"_annotations.coco.json\")\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.processor = processor\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # COCO 형식의 PIL 이미지와 타겟을 읽음\n",
    "            # 다음 단계로 전달하기 전에 여기서 데이터 증강\n",
    "            img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "            # 이미지와 타겟 전처리 (타겟을 DETR 포맷으로 변환, 이미지와 타겟 모두 크기 조정 및 정규화)\n",
    "            image_id = self.ids[idx]\n",
    "            target = {'image_id': image_id, 'annotations': target}\n",
    "            encoding = self.processor(images=img, annotations=target, return_tensors='pt')\n",
    "            pixel_values = encoding[\"pixel_values\"].squeeze()   # 배치 차원 제거\n",
    "            target = encoding['labels'][0]  # 배치 차원 제거\n",
    "            return pixel_values, target \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6da14e",
   "metadata": {},
   "source": [
    "- COCODetection - CustomDataset 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d8bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.37s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training examples: 6469\n",
      "Number of validation examples: 547\n"
     ]
    }
   ],
   "source": [
    "# 전처리를 위해서 DERTImageProcessor 사용\n",
    "\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "processor_DETR = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")  # DERT용 이미지 프로세서\n",
    "\n",
    "train_dataset = CocoDetection(img_folder='C:\\\\Users\\\\SVT\\\\Desktop\\\\PyTorch\\\\datasets\\\\train', processor=processor_DETR)\n",
    "val_dataset = CocoDetection(img_folder='C:\\\\Users\\\\SVT\\\\Desktop\\\\PyTorch\\\\datasets\\\\valid', processor=processor_DETR, train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17cf9ed",
   "metadata": {},
   "source": [
    "**Pytorch Dataloader 수행**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4644ec8",
   "metadata": {},
   "source": [
    "- collate_fn : batch_size로 묶인 데이터 각각을 같은 길이로 padding 하는 코드\n",
    "    - batch_size=1 : 길이와 상관 없이 적용하지만 2 이상이라면 모든 데이터의 길이는 같지 않음, 따라서 오류 발생\n",
    "    - 이를 방지하기 위해 collate_fn 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:626: UserWarning: This DataLoader will create 79 worker processes in total. Our suggested max number of worker in current system is 24 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = processor_DETR.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    batch = {}\n",
    "    batch['pixel_values'] = encoding['pixel_values']\n",
    "    batch['pixel_mask'] = encoding['pixel_mask']\n",
    "    batch['labels'] = labels\n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True, num_workers=79)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2, shuffle=False, num_workers=79)\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print('batch_key :',batch.keys())\n",
    "print()\n",
    "pixel_values, target=train_dataset[0]\n",
    "print(pixel_values.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed794ce",
   "metadata": {},
   "source": [
    "**Pytorch Lightning 수행**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69531d63",
   "metadata": {},
   "source": [
    "- pytorch lightning : High-level 인터페이스를 제공하는 오픈소스 Python 라이브러리 (정돈된 느낌으로 작성)\n",
    "    - ignore_mismatched_sizes=True 로 설정해야함\n",
    "    - pretrained 되어있는 class 에 맞지 않아도 알아서 맞춰주는 파라미터 인자 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb69069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "    def __init__(self, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "        # COCO 분류 헤드를 사용자 지정 헤드로 교체\n",
    "        # timm 라이브러리에 의존하지 않도록 \"no_timm\"변형 지정\n",
    "        # 합성곱 백본의 경우엔\n",
    "        # num_labels = 클래스 수\n",
    "        # ignore_mismatched_sizes : 모델 출력과 대상 크기 간 불일치를 무시할지 여부\n",
    "        self.model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\", id2label=id2label, num_labels=len(id2label), ignore_mismatched_sizes=True)\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "        return outputs\n",
    "    \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        pixel_mask = batch['pixel_mask']\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch['labels']]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "        return loss, loss_dict\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # 각 training_step 에 대한 로그 기록 및 에포크 전체의 평균\n",
    "        self.log('training_loss', loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log('train_' + k, v.item())\n",
    "\n",
    "            return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log('validation_loss', loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log('validation_' + k, v.item())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [ p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "             \"lr\": self.lr_backbone},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "    \n",
    "model_detr = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "outputs = model_detr(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n",
    "\n",
    "# 모델의 구조를 수정하거나 했을 때 구조 확인을 위한 출력\n",
    "print('model state_dict keys ')\n",
    "for i in model_detr.state_dict().keys():\n",
    "    print(i)\n",
    "\n",
    "# 총 파라미터수/ 학습가능한 파라미터수/ 고정된 파라미터수\n",
    "total_params = sum(p.numel() for p in model_detr.parameters())\n",
    "print(\"Total Parameters:\", total_params)\n",
    "\n",
    "total_trainable_params = sum(p.numel() for p in model_detr.parameters() if p.requires_grad)\n",
    "print(\"Total Trainable Parameters:\", total_trainable_params)\n",
    "\n",
    "total_fixed_params = sum(p.numel() for p in model_detr.parameters() if not p.requires_grad)\n",
    "print(\"Total Fixed Parameters:\", total_fixed_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67289a00",
   "metadata": {},
   "source": [
    "**Training**\n",
    "- early_stopping 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "\n",
    "## early stopping 적용 코드\n",
    "# from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "# early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"validation_loss\", min_delta=0.00, patience=3, verbose=False, mode=\"min\")\n",
    "# trainer = Trainer(accelerator=\"gpu\", devices=[3], max_epochs=30, callbacks=[early_step_callback], gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "# trainer.fit(model_detr)\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=[0], max_epochs=150, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "trainer.fit(model_detr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267af93",
   "metadata": {},
   "source": [
    "**Huggingface 에 올리기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a49c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface-cli login\n",
    "\n",
    "model_detr.model.push_to_hub(\"name/detr_custom\")\n",
    "processor_DETR.push_to_hub(\"name/detr_custom\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3b917",
   "metadata": {},
   "source": [
    "**모델 평가(Evaluation)**\n",
    "- HuggingFace 에서 작성했던 디렉토리에 맞게 주소 입력해서 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "model = DetrForObjectDetection.from_pretrained(\"name/detr_custom\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "processor = DetrImageProcessor.from_pretrained(\"name/detr_custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00408dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_xywh(boxes):\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim = 1)\n",
    "\n",
    "def prepare_for_coco_detection(predictions):\n",
    "    coco_results = []\n",
    "    for original_id, prediction in predictions.items():\n",
    "        if len(prediction) == 0:\n",
    "            continue\n",
    "\n",
    "        boxes = prediction['boxes']\n",
    "        boxes = convert_to_xywh(boxes).tolist()\n",
    "        scores = prediction['scores'].tolist()\n",
    "        labels = prediction['labels'].tolist()\n",
    "\n",
    "        coco_results.extend(\n",
    "            [\n",
    "                {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                }\n",
    "                for k, box in enumerate(boxes)\n",
    "            ]\n",
    "        )\n",
    "    return coco_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13522e44",
   "metadata": {},
   "source": [
    "- 평가 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# initialize evaluator with ground truth (gt)\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # DETR 형식이고, 크기 조정 및 정규화 됨\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "    # 사전 목록으로 변환(배치의 각 예제에 대해 하나의 항목)\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim = 0)\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes, threshold=0)\n",
    "\n",
    "    # 메트릭에 제공\n",
    "    # 메트릭은 사전 목록을 기대, 각 항목은 image_id, category_id, bbox 및 score 키 포함\n",
    "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    predictions = prepare_for_coco_detection(predictions)\n",
    "    evaluator.update(predictions)\n",
    "\n",
    "evaluator.synchronize_between_processes()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897d7a8",
   "metadata": {},
   "source": [
    "**Inference(시각화)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96530a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 val_dataset 가져옴 : val_dataset[1]\n",
    "pixel_values, target = val_dataset[1]\n",
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # forward pass to get class logits and bounding boxes\n",
    "    outputs = model(pixel_values=pixel_values, pixel_mask=None)\n",
    "print(\"Outputs:\", outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results(pil_img, scores, labels, boxes):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for score, label, (xmin, ymin, xmax, ymax), c in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n",
    "\n",
    "        text = f\"{model.config.id2label[label]}: {score:0.2f}\"\n",
    "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image based on ID\n",
    "image_id = target['image_id'].item()\n",
    "image = val_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join('C:\\Users\\SVT\\Desktop\\PyTorch\\datasets\\valid'), image['file_name'])\n",
    "\n",
    "# postprocess model outputs\n",
    "width, height = image.size\n",
    "postprocessed_outputs = processor.post_process_object_detection(outputs, taret_sizes=[(height, width)],\n",
    "                                                                threshold=0.9)\n",
    "results = postprocessed_outputs[0]\n",
    "plot_results(image, results['scores'], results['labels'], results['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e275f",
   "metadata": {},
   "source": [
    "**기타**\n",
    "- Params\n",
    "    - Trainable params : Encoder, Decoder, FFN\n",
    "    - Non-trainable params : ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e05072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
