{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0e5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7847e7ca",
   "metadata": {},
   "source": [
    "- 분류(Classification) 기본 훈련 루프 + TensorBoard 로깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4a3791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.0\n",
      "1.11.0\n",
      "11.3\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "print(torchvision.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aaa561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['1']\n",
      "[INFO] #Images: 216\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FasterRCNN.__init__() got an unexpected keyword argument 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 203\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# =====================================\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# 4) Model / Optim\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# =====================================\u001b[39;00m\n\u001b[0;32m    202\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(unique_classes)  \u001b[38;5;66;03m# background + K\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasterrcnn_resnet50_fpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDEFAULT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    206\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py:396\u001b[0m, in \u001b[0;36mfasterrcnn_resnet50_fpn\u001b[1;34m(pretrained, progress, num_classes, pretrained_backbone, trainable_backbone_layers, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m backbone \u001b[38;5;241m=\u001b[39m resnet50(pretrained\u001b[38;5;241m=\u001b[39mpretrained_backbone, progress\u001b[38;5;241m=\u001b[39mprogress, norm_layer\u001b[38;5;241m=\u001b[39mmisc_nn_ops\u001b[38;5;241m.\u001b[39mFrozenBatchNorm2d)\n\u001b[0;32m    395\u001b[0m backbone \u001b[38;5;241m=\u001b[39m _resnet_fpn_extractor(backbone, trainable_backbone_layers)\n\u001b[1;32m--> 396\u001b[0m model \u001b[38;5;241m=\u001b[39m FasterRCNN(backbone, num_classes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m    398\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m load_state_dict_from_url(model_urls[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasterrcnn_resnet50_fpn_coco\u001b[39m\u001b[38;5;124m\"\u001b[39m], progress\u001b[38;5;241m=\u001b[39mprogress)\n",
      "\u001b[1;31mTypeError\u001b[0m: FasterRCNN.__init__() got an unexpected keyword argument 'weights'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# =====================\n",
    "# CONFIG (edit here)\n",
    "# =====================\n",
    "DATA_DIR = r\"D:\\AI_SVT_Training_mk\\annotations\\annos\"   # jpg/png + VOC xml가 함께 있는 폴더\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"  # TensorBoard 로그 + 체크포인트 출력 기본 폴더\n",
    "RUN_NAME = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "LOGDIR = os.path.join(LOG_BASE, RUN_NAME)\n",
    "CKPT_DIR = os.path.join(LOGDIR, \"checkpoints\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4                 # GPU 메모리에 맞게 조절\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "VAL_SPLIT = 0.1                # 10% 검증\n",
    "MIN_BOX_SIZE = 1               # 너무 작은 박스 제거(px)\n",
    "SAVE_BEST_ONLY = True\n",
    "SEED = 42\n",
    "\n",
    "# =====================================\n",
    "# Utility: set seed\n",
    "# =====================================\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================\n",
    "# 1) Scan XMLs and build class map\n",
    "# =====================================\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "def find_pairs(data_dir):\n",
    "    xmls = sorted(glob.glob(os.path.join(data_dir, \"*.xml\")))\n",
    "    pairs = []\n",
    "    for xml in xmls:\n",
    "        stem = os.path.splitext(os.path.basename(xml))[0]\n",
    "        img_path = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = os.path.join(data_dir, stem + ext)\n",
    "            if os.path.exists(cand):\n",
    "                img_path = cand\n",
    "                break\n",
    "        if img_path is not None:\n",
    "            pairs.append((img_path, xml))\n",
    "    return pairs\n",
    "\n",
    "pairs = find_pairs(DATA_DIR)\n",
    "if len(pairs) == 0:\n",
    "    raise FileNotFoundError(f\"No (image, xml) pairs in: {DATA_DIR}\")\n",
    "\n",
    "# gather class names\n",
    "class_names = []\n",
    "for _, xml in pairs:\n",
    "    try:\n",
    "        root = ET.parse(xml).getroot()\n",
    "    except Exception:\n",
    "        continue\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\")\n",
    "        if name:\n",
    "            class_names.append(name.strip())\n",
    "\n",
    "if not class_names:\n",
    "    raise RuntimeError(\"No object classes found in XMLs.\")\n",
    "\n",
    "# unique + stable ordering\n",
    "unique_classes = sorted(set(class_names))\n",
    "# background=0 rule (torchvision detection)\n",
    "class_to_idx = {c: i + 1 for i, c in enumerate(unique_classes)}\n",
    "idx_to_class = {i + 1: c for i, c in enumerate(unique_classes)}\n",
    "\n",
    "# save classes\n",
    "with open(os.path.join(LOGDIR, \"classes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in unique_classes:\n",
    "        f.write(f\"{c}\\n\")\n",
    "\n",
    "print(\"[INFO] Classes:\", unique_classes)\n",
    "print(\"[INFO] #Images:\", len(pairs))\n",
    "\n",
    "# =====================================\n",
    "# 2) Dataset\n",
    "# =====================================\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, xml_path = self.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.findtext(\"name\").strip()\n",
    "            if name not in class_to_idx:\n",
    "                # unseen class -> skip\n",
    "                continue\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            if bnd is None:\n",
    "                continue\n",
    "            try:\n",
    "                xmin = float(bnd.findtext(\"xmin\"))\n",
    "                ymin = float(bnd.findtext(\"ymin\"))\n",
    "                xmax = float(bnd.findtext(\"xmax\"))\n",
    "                ymax = float(bnd.findtext(\"ymax\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # clamp\n",
    "            xmin = max(0, min(xmin, w - 1))\n",
    "            ymin = max(0, min(ymin, h - 1))\n",
    "            xmax = max(0, min(xmax, w - 1))\n",
    "            ymax = max(0, min(ymax, h - 1))\n",
    "\n",
    "            if xmax - xmin < MIN_BOX_SIZE or ymax - ymin < MIN_BOX_SIZE:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_to_idx[name])\n",
    "\n",
    "            diff = obj.findtext(\"difficult\")\n",
    "            iscrowd.append(1 if (diff and diff.strip() == \"1\") else 0)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            # torchvision detection은 타겟 비어있으면 학습에 문제 -> 샘플 건너뛸 수 없으니 최소 더미는 허용 X.\n",
    "            # 대신, 빈 샘플이면 작은 더미를 넣지 말고 Exception으로 처리해서 DataLoader에서 재시도하도록 함\n",
    "            raise ValueError(\"Empty target (no boxes)\")\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # to tensor [0,1]\n",
    "        img_t = F.pil_to_tensor(img).float() / 255.0\n",
    "\n",
    "        if self.transforms:\n",
    "            img_t = self.transforms(img_t)\n",
    "\n",
    "        return img_t, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # skip samples that raised ValueError (Empty target)\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    imgs, tgts = list(zip(*batch))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# =====================================\n",
    "# 3) Split train/val\n",
    "# =====================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=VAL_SPLIT, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = VOCDataset(train_pairs)\n",
    "val_ds   = VOCDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# 4) Model / Optim\n",
    "# =====================================\n",
    "num_classes = 1 + len(unique_classes)  # background + K\n",
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\", num_classes=num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=LR)\n",
    "\n",
    "# =====================================\n",
    "# 5) TensorBoard writer\n",
    "# =====================================\n",
    "writer = SummaryWriter(LOGDIR)\n",
    "with open(os.path.join(LOGDIR, \"readme.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"DATA_DIR={DATA_DIR}\\n\")\n",
    "    f.write(f\"classes={unique_classes}\\n\")\n",
    "\n",
    "# =====================================\n",
    "# 6) Train/Eval loops\n",
    "# =====================================\n",
    "\n",
    "def evaluate_loss(model, loader, max_iters=None):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for i, (images, targets) in enumerate(pbar):\n",
    "            images = [im.to(device) for im in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(v for v in loss_dict.values())\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "            if max_iters and i + 1 >= max_iters:\n",
    "                break\n",
    "    return total / max(1, count)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")\n",
    "    for images, targets in pbar:\n",
    "        images = [im.to(device) for im in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TB per-step\n",
    "        writer.add_scalar(\"train/total_loss\", loss.item(), step)\n",
    "        for k, v in loss_dict.items():\n",
    "            writer.add_scalar(f\"train/{k}\", v.item(), step)\n",
    "        step += 1\n",
    "\n",
    "        pbar.set_postfix(total_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_loss(model, val_loader)\n",
    "    writer.add_scalar(\"val/total_loss\", val_loss, epoch)\n",
    "\n",
    "    # Save\n",
    "    ckpt_path = os.path.join(CKPT_DIR, f\"epoch{epoch:03d}_valloss{val_loss:.4f}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"classes\": unique_classes,\n",
    "        \"val_loss\": val_loss,\n",
    "    }, ckpt_path)\n",
    "\n",
    "    if SAVE_BEST_ONLY:\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"best.pt\"))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"last.pt\"))\n",
    "\n",
    "# 훈련 종료 마커 (GUI가 감지하도록 텍스트 로그 남김)\n",
    "writer.add_text(\"status/final\", f\"done@{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "writer.close()\n",
    "\n",
    "print(f\"[DONE] Logs: {LOGDIR}\")\n",
    "print(f\"[TIP] tensorboard --logdir={LOG_BASE} --port=6007\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe15bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['1']\n",
      "[INFO] #Images(valid): 216 / total 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/20:   0%|          | 0/97 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13168, 19424, 17148, 13540) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 254\u001b[0m\n\u001b[0;32m    252\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    253\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    255\u001b[0m     images \u001b[38;5;241m=\u001b[39m [im\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    256\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1163\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1165\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1024\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1023\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 13168, 19424, 17148, 13540) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# CONFIG (edit here)\n",
    "# =====================\n",
    "DATA_DIR = r\"D:\\AI_SVT_Training_mk\\annotations\\annos\"   # jpg/png + VOC xml가 함께 있는 폴더\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"  # TensorBoard 로그 + 체크포인트 출력 기본 폴더\n",
    "RUN_NAME = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "LOGDIR = os.path.join(LOG_BASE, RUN_NAME)\n",
    "CKPT_DIR = os.path.join(LOGDIR, \"checkpoints\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 2                 # GPU 메모리에 맞게 조절\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 4\n",
    "VAL_SPLIT = 0.1                # 10% 검증\n",
    "MIN_BOX_SIZE = 1               # 너무 작은 박스 제거(px)\n",
    "SAVE_BEST_ONLY = True\n",
    "SEED = 42\n",
    "\n",
    "# =====================================\n",
    "# Utility: set seed\n",
    "# =====================================\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "def find_pairs(data_dir: str) -> List[Tuple[str, str]]:\n",
    "    xmls = sorted(glob.glob(os.path.join(data_dir, \"*.xml\")))\n",
    "    pairs = []\n",
    "    for xml in xmls:\n",
    "        stem = os.path.splitext(os.path.basename(xml))[0]\n",
    "        img_path = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = os.path.join(data_dir, stem + ext)\n",
    "            if os.path.exists(cand):\n",
    "                img_path = cand\n",
    "                break\n",
    "        if img_path is not None:\n",
    "            pairs.append((img_path, xml))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def xml_has_valid_object(xml_path: str) -> bool:\n",
    "    try:\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "    except Exception:\n",
    "        return False\n",
    "    for obj in root.findall(\"object\"):\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        if bnd is None:\n",
    "            continue\n",
    "        try:\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        if (xmax - xmin) >= MIN_BOX_SIZE and (ymax - ymin) >= MIN_BOX_SIZE:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "pairs_all = find_pairs(DATA_DIR)\n",
    "# 학습 중 빈 타겟 샘플로 인한 크래시를 방지하기 위해, 미리 필터링\n",
    "pairs = [(img, xml) for (img, xml) in pairs_all if xml_has_valid_object(xml)]\n",
    "if len(pairs) == 0:\n",
    "    raise FileNotFoundError(f\"No valid (image, xml) pairs with at least one box in: {DATA_DIR}\")\n",
    "\n",
    "# gather class names from filtered pairs만 사용\n",
    "class_names = []\n",
    "for _, xml in pairs:\n",
    "    root = ET.parse(xml).getroot()\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\")\n",
    "        if name:\n",
    "            class_names.append(name.strip())\n",
    "\n",
    "unique_classes = sorted(set(class_names))\n",
    "if not unique_classes:\n",
    "    raise RuntimeError(\"No object classes found in XMLs after filtering.\")\n",
    "\n",
    "# background=0 rule (torchvision detection)\n",
    "class_to_idx = {c: i + 1 for i, c in enumerate(unique_classes)}\n",
    "idx_to_class = {i + 1: c for i, c in enumerate(unique_classes)}\n",
    "\n",
    "with open(os.path.join(LOGDIR, \"classes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in unique_classes:\n",
    "        f.write(f\"{c}\\n\")\n",
    "\n",
    "print(\"[INFO] Classes:\", unique_classes)\n",
    "print(\"[INFO] #Images(valid):\", len(pairs), f\"/ total {len(pairs_all)}\")\n",
    "\n",
    "# =====================================\n",
    "# Dataset\n",
    "# =====================================\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, xml_path = self.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.findtext(\"name\").strip()\n",
    "            if name not in class_to_idx:\n",
    "                continue\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            if bnd is None:\n",
    "                continue\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "\n",
    "            # clamp\n",
    "            xmin = max(0, min(xmin, w - 1))\n",
    "            ymin = max(0, min(ymin, h - 1))\n",
    "            xmax = max(0, min(xmax, w - 1))\n",
    "            ymax = max(0, min(ymax, h - 1))\n",
    "\n",
    "            if xmax - xmin < MIN_BOX_SIZE or ymax - ymin < MIN_BOX_SIZE:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_to_idx[name])\n",
    "\n",
    "            diff = obj.findtext(\"difficult\")\n",
    "            iscrowd.append(1 if (diff and diff.strip() == \"1\") else 0)\n",
    "\n",
    "        # 0개 박스 샘플은 미리 필터링했으므로 여기선 반드시 >=1\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img_t = F.pil_to_tensor(img).float() / 255.0\n",
    "        if self.transforms:\n",
    "            img_t = self.transforms(img_t)\n",
    "        return img_t, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts = list(zip(*batch))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# =====================================\n",
    "# Split train/val\n",
    "# =====================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=VAL_SPLIT, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = VOCDataset(train_pairs)\n",
    "val_ds   = VOCDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# Model / Optim (LEGACY API for torchvision 0.12)\n",
    "# =====================================\n",
    "num_classes = 1 + len(unique_classes)  # background + K\n",
    "\n",
    "# 구버전: weights 인자 없음. COCO 사전학습 가중치 로드 → head 교체\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)  # backbone+RPN+head가 COCO용\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  # 새 head로 교체\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=LR)\n",
    "\n",
    "# =====================================\n",
    "# TensorBoard writer\n",
    "# =====================================\n",
    "writer = SummaryWriter(LOGDIR)\n",
    "with open(os.path.join(LOGDIR, \"readme.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"DATA_DIR={DATA_DIR}\\n\")\n",
    "    f.write(f\"classes={unique_classes}\\n\")\n",
    "    f.write(\"legacy api: torchvision 0.12 / torch 1.11\\n\")\n",
    "\n",
    "# =====================================\n",
    "# Train/Eval loops\n",
    "# =====================================\n",
    "\n",
    "def evaluate_loss(model, loader, max_iters=None):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for i, (images, targets) in enumerate(pbar):\n",
    "            images = [im.to(device) for im in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(v for v in loss_dict.values())\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "            if max_iters and i + 1 >= max_iters:\n",
    "                break\n",
    "    return total / max(1, count)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")\n",
    "    for images, targets in pbar:\n",
    "        images = [im.to(device) for im in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TB per-step\n",
    "        writer.add_scalar(\"train/total_loss\", loss.item(), step)\n",
    "        for k, v in loss_dict.items():\n",
    "            writer.add_scalar(f\"train/{k}\", v.item(), step)\n",
    "        step += 1\n",
    "\n",
    "        pbar.set_postfix(total_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_loss(model, val_loader)\n",
    "    writer.add_scalar(\"val/total_loss\", val_loss, epoch)\n",
    "\n",
    "    # Save\n",
    "    ckpt_path = os.path.join(CKPT_DIR, f\"epoch{epoch:03d}_valloss{val_loss:.4f}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"classes\": unique_classes,\n",
    "        \"val_loss\": val_loss,\n",
    "    }, ckpt_path)\n",
    "\n",
    "    if SAVE_BEST_ONLY:\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"best.pt\"))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"last.pt\"))\n",
    "\n",
    "# 훈련 종료 마커 (GUI가 감지하도록 텍스트 로그 남김)\n",
    "writer.add_text(\"status/final\", f\"done@{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "writer.close()\n",
    "\n",
    "print(f\"[DONE] Logs: {LOGDIR}\")\n",
    "print(f\"[TIP] tensorboard --logdir={LOG_BASE} --port=6007\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a917f797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['1']\n",
      "[INFO] #Images(valid): 216 / total 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/20: 100%|██████████| 49/49 [00:33<00:00,  1.46it/s, total_loss=0.1710]\n",
      "                                          \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 275\u001b[0m\n\u001b[0;32m    272\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(total_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval/total_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_loss, epoch)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 242\u001b[0m, in \u001b[0;36mevaluate_loss\u001b[1;34m(model, loader, max_iters)\u001b[0m\n\u001b[0;32m    240\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m    241\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n\u001b[1;32m--> 242\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloss_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m())\n\u001b[0;32m    243\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    244\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# CONFIG (edit here)\n",
    "# =====================\n",
    "DATA_DIR = r\"D:\\AI_SVT_Training_mk\\annotations\\annos\"   # jpg/png + VOC xml가 함께 있는 폴더\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"  # TensorBoard 로그 + 체크포인트 출력 기본 폴더\n",
    "RUN_NAME = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "LOGDIR = os.path.join(LOG_BASE, RUN_NAME)\n",
    "CKPT_DIR = os.path.join(LOGDIR, \"checkpoints\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4                 # GPU 메모리에 맞게 조절\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 0  # Jupyter/Windows에서 DataLoader 다중 프로세스 충돌 방지. 스크립트 실행 시 4~8로 올리세요.\n",
    "VAL_SPLIT = 0.1                # 10% 검증\n",
    "MIN_BOX_SIZE = 1               # 너무 작은 박스 제거(px)\n",
    "SAVE_BEST_ONLY = True\n",
    "SEED = 42\n",
    "\n",
    "# =====================================\n",
    "# Utility: set seed\n",
    "# =====================================\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "def find_pairs(data_dir: str) -> List[Tuple[str, str]]:\n",
    "    xmls = sorted(glob.glob(os.path.join(data_dir, \"*.xml\")))\n",
    "    pairs = []\n",
    "    for xml in xmls:\n",
    "        stem = os.path.splitext(os.path.basename(xml))[0]\n",
    "        img_path = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = os.path.join(data_dir, stem + ext)\n",
    "            if os.path.exists(cand):\n",
    "                img_path = cand\n",
    "                break\n",
    "        if img_path is not None:\n",
    "            pairs.append((img_path, xml))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def xml_has_valid_object(xml_path: str) -> bool:\n",
    "    try:\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "    except Exception:\n",
    "        return False\n",
    "    for obj in root.findall(\"object\"):\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        if bnd is None:\n",
    "            continue\n",
    "        try:\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        if (xmax - xmin) >= MIN_BOX_SIZE and (ymax - ymin) >= MIN_BOX_SIZE:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "pairs_all = find_pairs(DATA_DIR)\n",
    "# 학습 중 빈 타겟 샘플로 인한 크래시를 방지하기 위해, 미리 필터링\n",
    "pairs = [(img, xml) for (img, xml) in pairs_all if xml_has_valid_object(xml)]\n",
    "if len(pairs) == 0:\n",
    "    raise FileNotFoundError(f\"No valid (image, xml) pairs with at least one box in: {DATA_DIR}\")\n",
    "\n",
    "# gather class names from filtered pairs만 사용\n",
    "class_names = []\n",
    "for _, xml in pairs:\n",
    "    root = ET.parse(xml).getroot()\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\")\n",
    "        if name:\n",
    "            class_names.append(name.strip())\n",
    "\n",
    "unique_classes = sorted(set(class_names))\n",
    "if not unique_classes:\n",
    "    raise RuntimeError(\"No object classes found in XMLs after filtering.\")\n",
    "\n",
    "# background=0 rule (torchvision detection)\n",
    "class_to_idx = {c: i + 1 for i, c in enumerate(unique_classes)}\n",
    "idx_to_class = {i + 1: c for i, c in enumerate(unique_classes)}\n",
    "\n",
    "with open(os.path.join(LOGDIR, \"classes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in unique_classes:\n",
    "        f.write(f\"{c}\\n\")\n",
    "\n",
    "print(\"[INFO] Classes:\", unique_classes)\n",
    "print(\"[INFO] #Images(valid):\", len(pairs), f\"/ total {len(pairs_all)}\")\n",
    "\n",
    "# =====================================\n",
    "# Dataset\n",
    "# =====================================\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, xml_path = self.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.findtext(\"name\").strip()\n",
    "            if name not in class_to_idx:\n",
    "                continue\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            if bnd is None:\n",
    "                continue\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "\n",
    "            # clamp\n",
    "            xmin = max(0, min(xmin, w - 1))\n",
    "            ymin = max(0, min(ymin, h - 1))\n",
    "            xmax = max(0, min(xmax, w - 1))\n",
    "            ymax = max(0, min(ymax, h - 1))\n",
    "\n",
    "            if xmax - xmin < MIN_BOX_SIZE or ymax - ymin < MIN_BOX_SIZE:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_to_idx[name])\n",
    "\n",
    "            diff = obj.findtext(\"difficult\")\n",
    "            iscrowd.append(1 if (diff and diff.strip() == \"1\") else 0)\n",
    "\n",
    "        # 0개 박스 샘플은 미리 필터링했으므로 여기선 반드시 >=1\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img_t = F.pil_to_tensor(img).float() / 255.0\n",
    "        if self.transforms:\n",
    "            img_t = self.transforms(img_t)\n",
    "        return img_t, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts = list(zip(*batch))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# =====================================\n",
    "# Split train/val\n",
    "# =====================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=VAL_SPLIT, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = VOCDataset(train_pairs)\n",
    "val_ds   = VOCDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# Model / Optim (LEGACY API for torchvision 0.12)\n",
    "# =====================================\n",
    "num_classes = 1 + len(unique_classes)  # background + K\n",
    "\n",
    "# 구버전: weights 인자 없음. COCO 사전학습 가중치 로드 → head 교체\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)  # backbone+RPN+head가 COCO용\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  # 새 head로 교체\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=LR)\n",
    "\n",
    "# =====================================\n",
    "# TensorBoard writer\n",
    "# =====================================\n",
    "writer = SummaryWriter(LOGDIR)\n",
    "with open(os.path.join(LOGDIR, \"readme.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"DATA_DIR={DATA_DIR}\\n\")\n",
    "    f.write(f\"classes={unique_classes}\\n\")\n",
    "    f.write(\"legacy api: torchvision 0.12 / torch 1.11\\n\")\n",
    "\n",
    "# =====================================\n",
    "# Train/Eval loops\n",
    "# =====================================\n",
    "\n",
    "def evaluate_loss(model, loader, max_iters=None):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for i, (images, targets) in enumerate(pbar):\n",
    "            images = [im.to(device) for im in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(v for v in loss_dict.values())\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "            if max_iters and i + 1 >= max_iters:\n",
    "                break\n",
    "    return total / max(1, count)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")\n",
    "    for images, targets in pbar:\n",
    "        images = [im.to(device) for im in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TB per-step\n",
    "        writer.add_scalar(\"train/total_loss\", loss.item(), step)\n",
    "        for k, v in loss_dict.items():\n",
    "            writer.add_scalar(f\"train/{k}\", v.item(), step)\n",
    "        step += 1\n",
    "\n",
    "        pbar.set_postfix(total_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_loss(model, val_loader)\n",
    "    writer.add_scalar(\"val/total_loss\", val_loss, epoch)\n",
    "\n",
    "    # Save\n",
    "    ckpt_path = os.path.join(CKPT_DIR, f\"epoch{epoch:03d}_valloss{val_loss:.4f}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"classes\": unique_classes,\n",
    "        \"val_loss\": val_loss,\n",
    "    }, ckpt_path)\n",
    "\n",
    "    if SAVE_BEST_ONLY:\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"best.pt\"))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"last.pt\"))\n",
    "\n",
    "# 훈련 종료 마커 (GUI가 감지하도록 텍스트 로그 남김)\n",
    "writer.add_text(\"status/final\", f\"done@{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "writer.close()\n",
    "\n",
    "print(f\"[DONE] Logs: {LOGDIR}\")\n",
    "print(f\"[TIP] tensorboard --logdir={LOG_BASE} --port=6007\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69e2e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['1']\n",
      "[INFO] #Images(valid): 216 / total 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/20: 100%|██████████| 49/49 [00:18<00:00,  2.68it/s, total_loss=0.1342]\n",
      "Train 2/20: 100%|██████████| 49/49 [00:18<00:00,  2.70it/s, total_loss=0.0895]\n",
      "Train 3/20: 100%|██████████| 49/49 [00:18<00:00,  2.65it/s, total_loss=0.0844]\n",
      "Train 4/20: 100%|██████████| 49/49 [00:18<00:00,  2.67it/s, total_loss=0.0577]\n",
      "Train 5/20: 100%|██████████| 49/49 [00:18<00:00,  2.68it/s, total_loss=0.0526]\n",
      "Train 6/20: 100%|██████████| 49/49 [00:18<00:00,  2.72it/s, total_loss=0.0860]\n",
      "Train 7/20: 100%|██████████| 49/49 [00:18<00:00,  2.72it/s, total_loss=0.0759]\n",
      "Train 8/20: 100%|██████████| 49/49 [00:18<00:00,  2.69it/s, total_loss=0.0468]\n",
      "Train 9/20: 100%|██████████| 49/49 [00:18<00:00,  2.69it/s, total_loss=0.0391]\n",
      "Train 10/20: 100%|██████████| 49/49 [00:18<00:00,  2.69it/s, total_loss=0.0358]\n",
      "Train 11/20: 100%|██████████| 49/49 [00:18<00:00,  2.69it/s, total_loss=0.0446]\n",
      "Train 12/20: 100%|██████████| 49/49 [00:17<00:00,  2.73it/s, total_loss=0.0306]\n",
      "Train 13/20: 100%|██████████| 49/49 [00:18<00:00,  2.68it/s, total_loss=0.0468]\n",
      "Train 14/20: 100%|██████████| 49/49 [00:18<00:00,  2.71it/s, total_loss=0.0479]\n",
      "Train 15/20: 100%|██████████| 49/49 [00:18<00:00,  2.70it/s, total_loss=0.0187]\n",
      "Train 16/20: 100%|██████████| 49/49 [00:18<00:00,  2.71it/s, total_loss=0.0358]\n",
      "Train 17/20: 100%|██████████| 49/49 [00:18<00:00,  2.71it/s, total_loss=0.0366]\n",
      "Train 18/20: 100%|██████████| 49/49 [00:18<00:00,  2.72it/s, total_loss=0.0193]\n",
      "Train 19/20: 100%|██████████| 49/49 [00:18<00:00,  2.71it/s, total_loss=0.0455]\n",
      "Train 20/20: 100%|██████████| 49/49 [00:18<00:00,  2.67it/s, total_loss=0.0352]\n",
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Logs: D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\\run_20251106_153757\n",
      "[TIP] tensorboard --logdir=D:\\AI_SVT_Training_mk\\train_result\\pytorch_det --port=6007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# CONFIG (edit here)\n",
    "# =====================\n",
    "DATA_DIR = r\"D:\\AI_SVT_Training_mk\\annotations\\annos\"   # jpg/png + VOC xml가 함께 있는 폴더\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"  # TensorBoard 로그 + 체크포인트 출력 기본 폴더\n",
    "RUN_NAME = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "LOGDIR = os.path.join(LOG_BASE, RUN_NAME)\n",
    "CKPT_DIR = os.path.join(LOGDIR, \"checkpoints\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4                 # GPU 메모리에 맞게 조절\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 0  # Jupyter/Windows에서 DataLoader 다중 프로세스 충돌 방지. 스크립트 실행 시 4~8로 올리세요.\n",
    "VAL_SPLIT = 0.1                # 10% 검증\n",
    "MIN_BOX_SIZE = 1               # 너무 작은 박스 제거(px)\n",
    "SAVE_BEST_ONLY = True\n",
    "SEED = 42\n",
    "\n",
    "# =====================================\n",
    "# Utility: set seed\n",
    "# =====================================\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "def find_pairs(data_dir: str) -> List[Tuple[str, str]]:\n",
    "    xmls = sorted(glob.glob(os.path.join(data_dir, \"*.xml\")))\n",
    "    pairs = []\n",
    "    for xml in xmls:\n",
    "        stem = os.path.splitext(os.path.basename(xml))[0]\n",
    "        img_path = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = os.path.join(data_dir, stem + ext)\n",
    "            if os.path.exists(cand):\n",
    "                img_path = cand\n",
    "                break\n",
    "        if img_path is not None:\n",
    "            pairs.append((img_path, xml))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def xml_has_valid_object(xml_path: str) -> bool:\n",
    "    try:\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "    except Exception:\n",
    "        return False\n",
    "    for obj in root.findall(\"object\"):\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        if bnd is None:\n",
    "            continue\n",
    "        try:\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        if (xmax - xmin) >= MIN_BOX_SIZE and (ymax - ymin) >= MIN_BOX_SIZE:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "pairs_all = find_pairs(DATA_DIR)\n",
    "# 학습 중 빈 타겟 샘플로 인한 크래시를 방지하기 위해, 미리 필터링\n",
    "pairs = [(img, xml) for (img, xml) in pairs_all if xml_has_valid_object(xml)]\n",
    "if len(pairs) == 0:\n",
    "    raise FileNotFoundError(f\"No valid (image, xml) pairs with at least one box in: {DATA_DIR}\")\n",
    "\n",
    "# gather class names from filtered pairs만 사용\n",
    "class_names = []\n",
    "for _, xml in pairs:\n",
    "    root = ET.parse(xml).getroot()\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\")\n",
    "        if name:\n",
    "            class_names.append(name.strip())\n",
    "\n",
    "unique_classes = sorted(set(class_names))\n",
    "if not unique_classes:\n",
    "    raise RuntimeError(\"No object classes found in XMLs after filtering.\")\n",
    "\n",
    "# background=0 rule (torchvision detection)\n",
    "class_to_idx = {c: i + 1 for i, c in enumerate(unique_classes)}\n",
    "idx_to_class = {i + 1: c for i, c in enumerate(unique_classes)}\n",
    "\n",
    "with open(os.path.join(LOGDIR, \"classes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in unique_classes:\n",
    "        f.write(f\"{c}\\n\")\n",
    "\n",
    "print(\"[INFO] Classes:\", unique_classes)\n",
    "print(\"[INFO] #Images(valid):\", len(pairs), f\"/ total {len(pairs_all)}\")\n",
    "\n",
    "# =====================================\n",
    "# Dataset\n",
    "# =====================================\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, xml_path = self.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.findtext(\"name\").strip()\n",
    "            if name not in class_to_idx:\n",
    "                continue\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            if bnd is None:\n",
    "                continue\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "\n",
    "            # clamp\n",
    "            xmin = max(0, min(xmin, w - 1))\n",
    "            ymin = max(0, min(ymin, h - 1))\n",
    "            xmax = max(0, min(xmax, w - 1))\n",
    "            ymax = max(0, min(ymax, h - 1))\n",
    "\n",
    "            if xmax - xmin < MIN_BOX_SIZE or ymax - ymin < MIN_BOX_SIZE:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_to_idx[name])\n",
    "\n",
    "            diff = obj.findtext(\"difficult\")\n",
    "            iscrowd.append(1 if (diff and diff.strip() == \"1\") else 0)\n",
    "\n",
    "        # 0개 박스 샘플은 미리 필터링했으므로 여기선 반드시 >=1\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img_t = F.pil_to_tensor(img).float() / 255.0\n",
    "        if self.transforms:\n",
    "            img_t = self.transforms(img_t)\n",
    "        return img_t, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts = list(zip(*batch))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# =====================================\n",
    "# Split train/val\n",
    "# =====================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=VAL_SPLIT, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = VOCDataset(train_pairs)\n",
    "val_ds   = VOCDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# Model / Optim (LEGACY API for torchvision 0.12)\n",
    "# =====================================\n",
    "num_classes = 1 + len(unique_classes)  # background + K\n",
    "\n",
    "# 구버전: weights 인자 없음. COCO 사전학습 가중치 로드 → head 교체\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)  # backbone+RPN+head가 COCO용\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  # 새 head로 교체\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=LR)\n",
    "\n",
    "# =====================================\n",
    "# TensorBoard writer\n",
    "# =====================================\n",
    "writer = SummaryWriter(LOGDIR)\n",
    "with open(os.path.join(LOGDIR, \"readme.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"DATA_DIR={DATA_DIR}\\n\")\n",
    "    f.write(f\"classes={unique_classes}\\n\")\n",
    "    f.write(\"legacy api: torchvision 0.12 / torch 1.11\\n\")\n",
    "\n",
    "# =====================================\n",
    "# Train/Eval loops\n",
    "# =====================================\n",
    "\n",
    "def evaluate_loss(model, loader, max_iters=None):\n",
    "    \"\"\"Compute validation loss for torchvision detection models on torch==1.11/vision==0.12.\n",
    "    IMPORTANT: losses are returned ONLY when the model is in train mode.\n",
    "    So we temporarily switch to train(), run forward under no_grad(), then restore mode.\n",
    "    \"\"\"\n",
    "    prev_training = model.training\n",
    "    model.train()  # to get loss dict instead of detections list\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for i, (images, targets) in enumerate(pbar):\n",
    "            images = [im.to(device) for im in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            # loss_dict is a dict in train() mode\n",
    "            loss = sum(v for v in loss_dict.values())\n",
    "            total += float(loss.item())\n",
    "            count += 1\n",
    "            if max_iters and i + 1 >= max_iters:\n",
    "                break\n",
    "    # restore previous mode\n",
    "    model.train(prev_training)\n",
    "    return total / max(1, count)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")\n",
    "    for images, targets in pbar:\n",
    "        images = [im.to(device) for im in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TB per-step\n",
    "        writer.add_scalar(\"train/total_loss\", loss.item(), step)\n",
    "        for k, v in loss_dict.items():\n",
    "            writer.add_scalar(f\"train/{k}\", v.item(), step)\n",
    "        step += 1\n",
    "\n",
    "        pbar.set_postfix(total_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_loss(model, val_loader)\n",
    "    writer.add_scalar(\"val/total_loss\", val_loss, epoch)\n",
    "\n",
    "    # Save\n",
    "    ckpt_path = os.path.join(CKPT_DIR, f\"epoch{epoch:03d}_valloss{val_loss:.4f}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"classes\": unique_classes,\n",
    "        \"val_loss\": val_loss,\n",
    "    }, ckpt_path)\n",
    "\n",
    "    if SAVE_BEST_ONLY:\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"best.pt\"))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"last.pt\"))\n",
    "\n",
    "# 훈련 종료 마커 (GUI가 감지하도록 텍스트 로그 남김)\n",
    "writer.add_text(\"status/final\", f\"done@{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "writer.close()\n",
    "\n",
    "print(f\"[DONE] Logs: {LOGDIR}\")\n",
    "print(f\"[TIP] tensorboard --logdir={LOG_BASE} --port=6007\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec36bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['1']\n",
      "[INFO] #Images(valid): 216 / total 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1/20: 100%|██████████| 49/49 [00:37<00:00,  1.32it/s, total_loss=0.1729]\n",
      "Train 2/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0773]\n",
      "Train 3/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0712]\n",
      "Train 4/20: 100%|██████████| 49/49 [00:17<00:00,  2.85it/s, total_loss=0.0713]\n",
      "Train 5/20: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s, total_loss=0.0387]\n",
      "Train 6/20: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s, total_loss=0.0539]\n",
      "Train 7/20: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s, total_loss=0.0788]\n",
      "Train 8/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0434]\n",
      "Train 9/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0415]\n",
      "Train 10/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0394]\n",
      "Train 11/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0387]\n",
      "Train 12/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0271]\n",
      "Train 13/20: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s, total_loss=0.0363]\n",
      "Train 14/20: 100%|██████████| 49/49 [00:17<00:00,  2.84it/s, total_loss=0.0259]\n",
      "Train 15/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0187]\n",
      "Train 16/20: 100%|██████████| 49/49 [00:17<00:00,  2.82it/s, total_loss=0.0213]\n",
      "Train 17/20: 100%|██████████| 49/49 [00:17<00:00,  2.84it/s, total_loss=0.0293]\n",
      "Train 18/20: 100%|██████████| 49/49 [00:17<00:00,  2.84it/s, total_loss=0.0324]\n",
      "Train 19/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0432]\n",
      "Train 20/20: 100%|██████████| 49/49 [00:17<00:00,  2.83it/s, total_loss=0.0458]\n",
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Logs: D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\\run_20251106_162958\n",
      "[TIP] tensorboard --logdir=D:\\AI_SVT_Training_mk\\train_result\\pytorch_det --port=6007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes, make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =====================\n",
    "# CONFIG (edit here)\n",
    "# =====================\n",
    "DATA_DIR = r\"D:\\AI_SVT_Training_mk\\annotations\\annos\"   # jpg/png + VOC xml가 함께 있는 폴더\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"  # TensorBoard 로그 + 체크포인트 출력 기본 폴더\n",
    "RUN_NAME = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "LOGDIR = os.path.join(LOG_BASE, RUN_NAME)\n",
    "CKPT_DIR = os.path.join(LOGDIR, \"checkpoints\")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 4                 # GPU 메모리에 맞게 조절\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 0  # Jupyter/Windows에서 DataLoader 다중 프로세스 충돌 방지. 스크립트 실행 시 4~8로 올리세요.\n",
    "VAL_SPLIT = 0.1                # 10% 검증\n",
    "MIN_BOX_SIZE = 1               # 너무 작은 박스 제거(px)\n",
    "SAVE_BEST_ONLY = True\n",
    "SEED = 42\n",
    "\n",
    "# =====================================\n",
    "# Utility: set seed\n",
    "# =====================================\n",
    "def set_seed(seed: int = 42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "\n",
    "def find_pairs(data_dir: str) -> List[Tuple[str, str]]:\n",
    "    xmls = sorted(glob.glob(os.path.join(data_dir, \"*.xml\")))\n",
    "    pairs = []\n",
    "    for xml in xmls:\n",
    "        stem = os.path.splitext(os.path.basename(xml))[0]\n",
    "        img_path = None\n",
    "        for ext in IMG_EXTS:\n",
    "            cand = os.path.join(data_dir, stem + ext)\n",
    "            if os.path.exists(cand):\n",
    "                img_path = cand\n",
    "                break\n",
    "        if img_path is not None:\n",
    "            pairs.append((img_path, xml))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def xml_has_valid_object(xml_path: str) -> bool:\n",
    "    try:\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "    except Exception:\n",
    "        return False\n",
    "    for obj in root.findall(\"object\"):\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        if bnd is None:\n",
    "            continue\n",
    "        try:\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        if (xmax - xmin) >= MIN_BOX_SIZE and (ymax - ymin) >= MIN_BOX_SIZE:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "pairs_all = find_pairs(DATA_DIR)\n",
    "# 학습 중 빈 타겟 샘플로 인한 크래시를 방지하기 위해, 미리 필터링\n",
    "pairs = [(img, xml) for (img, xml) in pairs_all if xml_has_valid_object(xml)]\n",
    "if len(pairs) == 0:\n",
    "    raise FileNotFoundError(f\"No valid (image, xml) pairs with at least one box in: {DATA_DIR}\")\n",
    "\n",
    "# gather class names from filtered pairs만 사용\n",
    "class_names = []\n",
    "for _, xml in pairs:\n",
    "    root = ET.parse(xml).getroot()\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\")\n",
    "        if name:\n",
    "            class_names.append(name.strip())\n",
    "\n",
    "unique_classes = sorted(set(class_names))\n",
    "if not unique_classes:\n",
    "    raise RuntimeError(\"No object classes found in XMLs after filtering.\")\n",
    "\n",
    "# background=0 rule (torchvision detection)\n",
    "class_to_idx = {c: i + 1 for i, c in enumerate(unique_classes)}\n",
    "idx_to_class = {i + 1: c for i, c in enumerate(unique_classes)}\n",
    "\n",
    "with open(os.path.join(LOGDIR, \"classes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in unique_classes:\n",
    "        f.write(f\"{c}\\n\")\n",
    "\n",
    "print(\"[INFO] Classes:\", unique_classes)\n",
    "print(\"[INFO] #Images(valid):\", len(pairs), f\"/ total {len(pairs_all)}\")\n",
    "\n",
    "# =====================================\n",
    "# Dataset\n",
    "# =====================================\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, pairs, transforms=None):\n",
    "        self.pairs = pairs\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, xml_path = self.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        iscrowd = []\n",
    "\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.findtext(\"name\").strip()\n",
    "            if name not in class_to_idx:\n",
    "                continue\n",
    "            bnd = obj.find(\"bndbox\")\n",
    "            if bnd is None:\n",
    "                continue\n",
    "            xmin = float(bnd.findtext(\"xmin\"))\n",
    "            ymin = float(bnd.findtext(\"ymin\"))\n",
    "            xmax = float(bnd.findtext(\"xmax\"))\n",
    "            ymax = float(bnd.findtext(\"ymax\"))\n",
    "\n",
    "            # clamp\n",
    "            xmin = max(0, min(xmin, w - 1))\n",
    "            ymin = max(0, min(ymin, h - 1))\n",
    "            xmax = max(0, min(xmax, w - 1))\n",
    "            ymax = max(0, min(ymax, h - 1))\n",
    "\n",
    "            if xmax - xmin < MIN_BOX_SIZE or ymax - ymin < MIN_BOX_SIZE:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(class_to_idx[name])\n",
    "\n",
    "            diff = obj.findtext(\"difficult\")\n",
    "            iscrowd.append(1 if (diff and diff.strip() == \"1\") else 0)\n",
    "\n",
    "        # 0개 박스 샘플은 미리 필터링했으므로 여기선 반드시 >=1\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"iscrowd\": iscrowd,\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img_t = F.pil_to_tensor(img).float() / 255.0\n",
    "        if self.transforms:\n",
    "            img_t = self.transforms(img_t)\n",
    "        return img_t, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tgts = list(zip(*batch))\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "# =====================================\n",
    "# Split train/val\n",
    "# =====================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pairs, val_pairs = train_test_split(pairs, test_size=VAL_SPLIT, random_state=SEED, shuffle=True)\n",
    "\n",
    "train_ds = VOCDataset(train_pairs)\n",
    "val_ds   = VOCDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# =====================================\n",
    "# Model / Optim (LEGACY API for torchvision 0.12)\n",
    "# =====================================\n",
    "num_classes = 1 + len(unique_classes)  # background + K\n",
    "\n",
    "# 구버전: weights 인자 없음. COCO 사전학습 가중치 로드 → head 교체\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)  # backbone+RPN+head가 COCO용\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  # 새 head로 교체\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=LR)\n",
    "\n",
    "# =====================================\n",
    "# TensorBoard writer\n",
    "# =====================================\n",
    "writer = SummaryWriter(LOGDIR)\n",
    "with open(os.path.join(LOGDIR, \"readme.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"DATA_DIR={DATA_DIR}\\n\")\n",
    "    f.write(f\"classes={unique_classes}\\n\")\n",
    "    f.write(\"legacy api: torchvision 0.12 / torch 1.11\\n\")\n",
    "\n",
    "# =====================================\n",
    "# Train/Eval loops\n",
    "# =====================================\n",
    "\n",
    "def evaluate_loss(model, loader, max_iters=None):\n",
    "    \"\"\"Compute validation loss for torchvision detection models on torch==1.11/vision==0.12.\n",
    "    IMPORTANT: losses are returned ONLY when the model is in train mode.\n",
    "    So we temporarily switch to train(), run forward under no_grad(), then restore mode.\n",
    "    \"\"\"\n",
    "    prev_training = model.training\n",
    "    model.train()  # to get loss dict instead of detections list\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for i, (images, targets) in enumerate(pbar):\n",
    "            images = [im.to(device) for im in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            # loss_dict is a dict in train() mode\n",
    "            loss = sum(v for v in loss_dict.values())\n",
    "            total += float(loss.item())\n",
    "            count += 1\n",
    "            if max_iters and i + 1 >= max_iters:\n",
    "                break\n",
    "    # restore previous mode\n",
    "    model.train(prev_training)\n",
    "    return total / max(1, count)\n",
    "\n",
    "\n",
    "def log_predictions_for_tb(model, loader, writer, epoch: int, device, score_thr: float = 0.5, max_images: int = 4):\n",
    "    \"\"\"Run model in eval() to get predictions and upload visualizations to TensorBoard.\n",
    "    - Filters by score >= score_thr\n",
    "    - Logs up to max_images images under tag 'predictions/*'\n",
    "    \"\"\"\n",
    "    prev_training = model.training\n",
    "    model.eval()\n",
    "    logged = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = [im.to(device) for im in images]\n",
    "            preds = model(images)  # list of dicts with boxes/scores/labels\n",
    "            for img_t, pred in zip(images, preds):\n",
    "                img = (img_t.clamp(0, 1) * 255).byte().cpu()\n",
    "                boxes = pred.get(\"boxes\", torch.empty((0, 4)))\n",
    "                scores = pred.get(\"scores\", torch.empty((0,)))\n",
    "                labels = pred.get(\"labels\", torch.empty((0,), dtype=torch.int64))\n",
    "                if boxes.numel() == 0:\n",
    "                    continue\n",
    "                keep = scores >= score_thr\n",
    "                boxes = boxes[keep].cpu()\n",
    "                labels = labels[keep].cpu()\n",
    "                scores = scores[keep].cpu()\n",
    "                if boxes.numel() == 0:\n",
    "                    continue\n",
    "                # prepare string labels with class names and scores\n",
    "                text_labels = []\n",
    "                for l, s in zip(labels.tolist(), scores.tolist()):\n",
    "                    name = idx_to_class.get(l, str(l))\n",
    "                    text_labels.append(f\"{name}:{s:.2f}\")\n",
    "                vis = draw_bounding_boxes(img, boxes=boxes, labels=text_labels, width=2)\n",
    "                writer.add_image(f\"predictions/val_image_{logged}\", vis, epoch)\n",
    "                logged += 1\n",
    "                if logged >= max_images:\n",
    "                    break\n",
    "            if logged >= max_images:\n",
    "                break\n",
    "    # restore previous mode\n",
    "    model.train(prev_training)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Train {epoch}/{EPOCHS}\")\n",
    "    for images, targets in pbar:\n",
    "        images = [im.to(device) for im in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TB per-step\n",
    "        writer.add_scalar(\"train/total_loss\", loss.item(), step)\n",
    "        for k, v in loss_dict.items():\n",
    "            writer.add_scalar(f\"train/{k}\", v.item(), step)\n",
    "        step += 1\n",
    "\n",
    "        pbar.set_postfix(total_loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss = evaluate_loss(model, val_loader)\n",
    "    writer.add_scalar(\"val/total_loss\", val_loss, epoch)\n",
    "\n",
    "    # Eval-mode predictions → TensorBoard 시각화 (상위 4장, score≥0.5)\n",
    "    log_predictions_for_tb(model, val_loader, writer, epoch, device, score_thr=0.5, max_images=4)\n",
    "\n",
    "    # Save\n",
    "    ckpt_path = os.path.join(CKPT_DIR, f\"epoch{epoch:03d}_valloss{val_loss:.4f}.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"classes\": unique_classes,\n",
    "        \"val_loss\": val_loss,\n",
    "    }, ckpt_path)\n",
    "\n",
    "    if SAVE_BEST_ONLY:\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"best.pt\"))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), os.path.join(CKPT_DIR, \"last.pt\"))\n",
    "\n",
    "# 훈련 종료 마커 (GUI가 감지하도록 텍스트 로그 남김)\n",
    "writer.add_text(\"status/final\", f\"done@{time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "writer.close()\n",
    "\n",
    "print(f\"[DONE] Logs: {LOGDIR}\")\n",
    "print(f\"[TIP] tensorboard --logdir={LOG_BASE} --port=6007\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2141586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Using run directory: D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\\run_20251106_162958\n",
      "[PICK] Best by 'val/total_loss': step=18, value=0.033118\n",
      "[CKPT] Using checkpoint: D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\\run_20251106_162958\\checkpoints\\best.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Checkpoint does not contain 'classes'. Re-train with the provided training script.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 233\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] Export complete. You can run inference_example.py to test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 233\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 226\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    223\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m locate_checkpoint(run_dir, best_epoch)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CKPT] Using checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m model, classes \u001b[38;5;241m=\u001b[39m \u001b[43mrebuild_model_from_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m export_dir \u001b[38;5;241m=\u001b[39m export_package(run_dir, ckpt_path, (best_step, best_val), model, classes)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] Export complete. You can run inference_example.py to test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 110\u001b[0m, in \u001b[0;36mrebuild_model_from_ckpt\u001b[1;34m(ckpt_path, device)\u001b[0m\n\u001b[0;32m    108\u001b[0m classes \u001b[38;5;241m=\u001b[39m ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint does not contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Re-train with the provided training script.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes)\n\u001b[0;32m    112\u001b[0m model \u001b[38;5;241m=\u001b[39m fasterrcnn_resnet50_fpn(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Checkpoint does not contain 'classes'. Re-train with the provided training script."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export the best PyTorch Faster R-CNN model picked by TensorBoard metrics (legacy torch==1.11 / tv==0.12).\n",
    "- Scans a run directory under LOG_BASE (or auto-picks the latest run) where the training script saved:\n",
    "  - TensorBoard scalars: `val/total_loss` (and possibly other metrics in the future)\n",
    "  - Checkpoints: `checkpoints/epochXXX_valloss*.pt` and (optionally) `checkpoints/best.pt`\n",
    "- Chooses the best epoch by lowest `val/total_loss` (if you later log mAP, you can flip the selector).\n",
    "- Rebuilds model with the correct num_classes using the `classes` saved in checkpoint.\n",
    "- Exports a lightweight deployment package:\n",
    "  * model_best.pt (state_dict)\n",
    "  * classes.txt\n",
    "  * best_epoch.json\n",
    "  * model_config.json\n",
    "  * inference_example.py  (simple example loader to run predictions on an images folder)\n",
    "\n",
    "NOTE: TorchScript/ONNX export of torchvision detection models on torch 1.11 / tv 0.12 is brittle.\n",
    "      This script intentionally exports state_dict and a reference inference script instead of JIT/ONNX.\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"  # parent folder containing run_YYYYMMDD_HHMMSS\n",
    "RUN_DIR  = None  # if None, auto-pick the most recently modified run under LOG_BASE\n",
    "SCALAR_KEY = \"val/total_loss\"  # metric to minimize\n",
    "EXPORT_NAME = \"exported_model\"  # subfolder name inside RUN_DIR\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ============================\n",
    "\n",
    "\n",
    "def pick_latest_run(base):\n",
    "    runs = []\n",
    "    for d in glob.glob(os.path.join(base, \"run_*\")):\n",
    "        if os.path.isdir(d):\n",
    "            mtime = os.path.getmtime(d)\n",
    "            runs.append((mtime, d))\n",
    "    if not runs:\n",
    "        raise FileNotFoundError(f\"No run_* directory found under {base}\")\n",
    "    runs.sort(reverse=True)\n",
    "    return runs[0][1]\n",
    "\n",
    "\n",
    "def find_event_files(run_dir):\n",
    "    event_files = glob.glob(os.path.join(run_dir, \"events.out.tfevents.*\"))\n",
    "    if not event_files:\n",
    "        # sometimes SummaryWriter creates subdirs; scan all\n",
    "        event_files = glob.glob(os.path.join(run_dir, \"**\", \"events.out.tfevents.*\"), recursive=True)\n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(f\"No TensorBoard event files found in {run_dir}\")\n",
    "    return event_files\n",
    "\n",
    "\n",
    "def load_scalar_series(event_file, tag):\n",
    "    ea = EventAccumulator(event_file)\n",
    "    ea.Reload()\n",
    "    if tag not in ea.Tags().get('scalars', []):\n",
    "        return []\n",
    "    return [(e.step, e.value) for e in ea.Scalars(tag)]\n",
    "\n",
    "\n",
    "def choose_best_epoch_by_scalar(event_files, tag):\n",
    "    \"\"\"Merge all TB files; take min value of the tag, return (best_step, best_value).\"\"\"\n",
    "    all_points = []\n",
    "    for ef in event_files:\n",
    "        pts = load_scalar_series(ef, tag)\n",
    "        all_points.extend(pts)\n",
    "    if not all_points:\n",
    "        raise RuntimeError(f\"No scalar data for tag '{tag}' in event files.\")\n",
    "    # deduplicate by step keeping the last value\n",
    "    by_step = {}\n",
    "    for step, val in all_points:\n",
    "        by_step[step] = val\n",
    "    best_step = min(by_step, key=lambda s: by_step[s])\n",
    "    return best_step, by_step[best_step]\n",
    "\n",
    "\n",
    "def locate_checkpoint(run_dir, best_epoch):\n",
    "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        raise FileNotFoundError(f\"Checkpoint dir not found: {ckpt_dir}\")\n",
    "    # direct best.pt takes precedence if exists\n",
    "    best_pt = os.path.join(ckpt_dir, \"best.pt\")\n",
    "    if os.path.exists(best_pt):\n",
    "        return best_pt\n",
    "    # otherwise pick epoch file\n",
    "    pattern = os.path.join(ckpt_dir, f\"epoch{best_epoch:03d}_valloss*.pt\")\n",
    "    matches = glob.glob(pattern)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    # fallback: last epoch file with the same epoch number (if naming changed)\n",
    "    pattern = os.path.join(ckpt_dir, f\"epoch{best_epoch:03d}_*.pt\")\n",
    "    matches = glob.glob(pattern)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    raise FileNotFoundError(f\"No checkpoint found for epoch {best_epoch} in {ckpt_dir}\")\n",
    "\n",
    "\n",
    "def rebuild_model_from_ckpt(ckpt_path, device=DEVICE):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    classes = ckpt.get(\"classes\")\n",
    "    if classes is None:\n",
    "        raise RuntimeError(\"Checkpoint does not contain 'classes'. Re-train with the provided training script.\")\n",
    "    num_classes = 1 + len(classes)\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    state = ckpt.get(\"model\") or ckpt  # support plain state_dict or wrapped\n",
    "    model.load_state_dict(state)\n",
    "    model.eval().to(device)\n",
    "    return model, classes\n",
    "\n",
    "\n",
    "def export_package(run_dir, ckpt_path, best_info, model, classes):\n",
    "    export_dir = os.path.join(run_dir, EXPORT_NAME)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # Save state_dict (pure)\n",
    "    state_path = os.path.join(export_dir, \"model_best.pt\")\n",
    "    torch.save(model.state_dict(), state_path)\n",
    "\n",
    "    # Save classes\n",
    "    classes_txt_src = os.path.join(run_dir, \"classes.txt\")\n",
    "    classes_txt_dst = os.path.join(export_dir, \"classes.txt\")\n",
    "    if os.path.exists(classes_txt_src):\n",
    "        shutil.copy2(classes_txt_src, classes_txt_dst)\n",
    "    else:\n",
    "        with open(classes_txt_dst, \"w\", encoding=\"utf-8\") as f:\n",
    "            for c in classes:\n",
    "                f.write(f\"{c}\\n\")\n",
    "\n",
    "    # Save metadata\n",
    "    meta = {\n",
    "        \"selected_by\": SCALAR_KEY,\n",
    "        \"best_step\": int(best_info[0]),\n",
    "        \"best_value\": float(best_info[1]),\n",
    "        \"checkpoint\": os.path.relpath(ckpt_path, run_dir),\n",
    "        \"export_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(os.path.join(export_dir, \"best_epoch.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(os.path.join(export_dir, \"model_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"num_classes\": 1 + len(classes), \"classes\": classes}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Write a tiny inference helper\n",
    "    infer_py = os.path.join(export_dir, \"inference_example.py\")\n",
    "    with open(infer_py, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\"\"# Minimal inference example for torchvision Faster R-CNN (torch 1.11 / tv 0.12)\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "DEVICE = '{DEVICE}'\n",
    "EXPORT_DIR = os.path.dirname(__file__)\n",
    "STATE_DICT = os.path.join(EXPORT_DIR, 'model_best.pt')\n",
    "CLASSES_TXT = os.path.join(EXPORT_DIR, 'classes.txt')\n",
    "\n",
    "# load classes\n",
    "classes = []\n",
    "with open(CLASSES_TXT, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        if line:\n",
    "            classes.append(line)\n",
    "num_classes = 1 + len(classes)\n",
    "\n",
    "# rebuild model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load(STATE_DICT, map_location='cpu'))\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, score_thr=0.5):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = F.pil_to_tensor(img).float()/255.0\n",
    "    pred = model([x.to(DEVICE)])[0]\n",
    "    keep = pred['scores'] >= score_thr\n",
    "    return {\n",
    "        'boxes': pred['boxes'][keep].cpu().tolist(),\n",
    "        'scores': pred['scores'][keep].cpu().tolist(),\n",
    "        'labels': [classes[int(l)-1] for l in pred['labels'][keep].cpu().tolist()],\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dir = os.path.join(EXPORT_DIR, 'test_images')\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    print(f\"Put test images in: {test_dir}\")\n",
    "    for img in glob.glob(os.path.join(test_dir, '*.*')):\n",
    "        try:\n",
    "            out = predict_image(img)\n",
    "            print(os.path.basename(img), out)\n",
    "        except Exception as e:\n",
    "            print('ERR', img, e)\n",
    "\"\"\")\n",
    "\n",
    "    print(f\"[EXPORT] Package written to: {export_dir}\")\n",
    "    return export_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    run_dir = RUN_DIR or pick_latest_run(LOG_BASE)\n",
    "    print(f\"[RUN] Using run directory: {run_dir}\")\n",
    "\n",
    "    event_files = find_event_files(run_dir)\n",
    "    best_step, best_val = choose_best_epoch_by_scalar(event_files, SCALAR_KEY)\n",
    "    print(f\"[PICK] Best by '{SCALAR_KEY}': step={best_step}, value={best_val:.6f}\")\n",
    "\n",
    "    # Our training logged val scalar per epoch with global_step==epoch\n",
    "    best_epoch = int(best_step)\n",
    "    ckpt_path = locate_checkpoint(run_dir, best_epoch)\n",
    "    print(f\"[CKPT] Using checkpoint: {ckpt_path}\")\n",
    "\n",
    "    model, classes = rebuild_model_from_ckpt(ckpt_path, device=DEVICE)\n",
    "    export_dir = export_package(run_dir, ckpt_path, (best_step, best_val), model, classes)\n",
    "\n",
    "    print(\"[DONE] Export complete. You can run inference_example.py to test.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a82993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\\run_20251106_162958\n",
      "[BEST] epoch (by val/total_loss min): 18\n",
      "[CKPT] D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\\run_20251106_162958\\checkpoints\\best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SVT\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3877: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  (torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i], dtype=torch.float32)).float()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TS] TorchScript export skipped (not supported on this build)\n",
      "[ONNX] Export skipped (package missing or export failed)\n",
      "[DONE] Exported to: D:\\AI_SVT_Training_mk\\output_inference_graph_pytorch\\saved_model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Make TF-style export folder for PyTorch Faster R-CNN (torch==1.11 / tv==0.12).\n",
    "Creates: D:\\AI_SVT_Training_mk\\output_inference_graph_pytorch\\saved_model\\\n",
    "\n",
    "Contents (always):\n",
    "  - model_state_dict.pt        (weights)\n",
    "  - classes.txt                (one label per line)\n",
    "  - label_map.pbtxt            (TF-style label map for compatibility)\n",
    "  - model_config.json          (num_classes, class names)\n",
    "  - inference.py               (CLI/func to run inference on an image or folder)\n",
    "  - README.txt                 (how to use)\n",
    "\n",
    "Optional (best-effort; guarded):\n",
    "  - model_scripted.ts          (TorchScript; may fail on this version)\n",
    "  - model.onnx                 (ONNX; only if onnx is installed and export passes)\n",
    "\n",
    "It selects the best checkpoint by reading TensorBoard scalars (val/total_loss),\n",
    "falling back to best.pt or the latest epoch file if needed.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# ========== CONFIG (edit) ==========\n",
    "LOG_BASE = r\"D:\\AI_SVT_Training_mk\\train_result\\pytorch_det\"\n",
    "OUTPUT_ROOT = r\"D:\\AI_SVT_Training_mk\\output_inference_graph_pytorch\"\n",
    "RUN_DIR = None               # None -> pick latest run under LOG_BASE\n",
    "SCALAR_KEY = \"val/total_loss\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ===================================\n",
    "\n",
    "\n",
    "def pick_latest_run(base):\n",
    "    runs = sorted(\n",
    "        [d for d in glob.glob(os.path.join(base, \"run_*\")) if os.path.isdir(d)],\n",
    "        key=lambda p: os.path.getmtime(p),\n",
    "        reverse=True,\n",
    "    )\n",
    "    if not runs:\n",
    "        raise FileNotFoundError(f\"No run_* directory found under {base}\")\n",
    "    return runs[0]\n",
    "\n",
    "\n",
    "def find_event_files(run_dir):\n",
    "    event_files = glob.glob(os.path.join(run_dir, \"events.out.tfevents.*\"))\n",
    "    if not event_files:\n",
    "        event_files = glob.glob(os.path.join(run_dir, \"**\", \"events.out.tfevents.*\"), recursive=True)\n",
    "    return event_files\n",
    "\n",
    "\n",
    "def choose_best_step(event_files, tag):\n",
    "    points = []\n",
    "    for ef in event_files:\n",
    "        ea = EventAccumulator(ef)\n",
    "        try:\n",
    "            ea.Reload()\n",
    "            if tag in ea.Tags().get('scalars', []):\n",
    "                points.extend((e.step, e.value) for e in ea.Scalars(tag))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not points:\n",
    "        return None\n",
    "    by_step = {}\n",
    "    for s, v in points:\n",
    "        by_step[s] = v\n",
    "    best = min(by_step, key=lambda s: by_step[s])\n",
    "    return int(best)\n",
    "\n",
    "\n",
    "def locate_checkpoint(run_dir, best_epoch):\n",
    "    ckpt_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "    if not os.path.isdir(ckpt_dir):\n",
    "        raise FileNotFoundError(f\"Checkpoint dir not found: {ckpt_dir}\")\n",
    "    cand = os.path.join(ckpt_dir, \"best.pt\")\n",
    "    if os.path.exists(cand):\n",
    "        return cand\n",
    "    if best_epoch is not None:\n",
    "        matches = glob.glob(os.path.join(ckpt_dir, f\"epoch{best_epoch:03d}_*.pt\"))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    # fallback: latest epoch file\n",
    "    epoch_files = sorted(glob.glob(os.path.join(ckpt_dir, \"epoch*_*.pt\")), key=os.path.getmtime)\n",
    "    if epoch_files:\n",
    "        return epoch_files[-1]\n",
    "    # fallback: any .pt\n",
    "    others = sorted(glob.glob(os.path.join(ckpt_dir, \"*.pt\")), key=os.path.getmtime)\n",
    "    if others:\n",
    "        return others[-1]\n",
    "    raise FileNotFoundError(\"No checkpoint .pt found in checkpoints/\")\n",
    "\n",
    "\n",
    "def read_classes(run_dir, state):\n",
    "    # 1) classes.txt\n",
    "    cls_txt = os.path.join(run_dir, \"classes.txt\")\n",
    "    if os.path.exists(cls_txt):\n",
    "        with open(cls_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    # 2) infer from state dict predictor head\n",
    "    if isinstance(state, dict):\n",
    "        for k in [\n",
    "            \"roi_heads.box_predictor.cls_score.weight\",\n",
    "            \"module.roi_heads.box_predictor.cls_score.weight\",\n",
    "        ]:\n",
    "            if k in state:\n",
    "                out_ch = state[k].shape[0]\n",
    "                k_classes = out_ch - 1  # background included\n",
    "                return [f\"class_{i}\" for i in range(1, k_classes + 1)]\n",
    "    return None\n",
    "\n",
    "\n",
    "def rebuild_model(num_classes, state_dict):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval().to(DEVICE)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_label_map_pbtxt(classes):\n",
    "    lines = []\n",
    "    for i, name in enumerate(classes, start=1):\n",
    "        lines.append(\"item {\")\n",
    "        lines.append(f\"  id: {i}\")\n",
    "        lines.append(f\"  name: '{name}'\")\n",
    "        lines.append(\"}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "\n",
    "def safe_torchscript(model, example):\n",
    "    \"\"\"Best-effort TorchScript. Returns path or None.\"\"\"\n",
    "    try:\n",
    "        scripted = torch.jit.trace(model, [example])  # tracing list input\n",
    "        return scripted\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def safe_export_onnx(model, example, out_path):\n",
    "    try:\n",
    "        import onnx  # noqa\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (example,),\n",
    "            out_path,\n",
    "            input_names=[\"images\"],\n",
    "            output_names=[\"detections\"],\n",
    "            opset_version=12,\n",
    "            do_constant_folding=True,\n",
    "        )\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    run_dir = RUN_DIR or pick_latest_run(LOG_BASE)\n",
    "    print(f\"[RUN] {run_dir}\")\n",
    "    event_files = find_event_files(run_dir)\n",
    "    best_epoch = choose_best_step(event_files, SCALAR_KEY)\n",
    "    print(f\"[BEST] epoch (by {SCALAR_KEY} min): {best_epoch}\")\n",
    "\n",
    "    ckpt_path = locate_checkpoint(run_dir, best_epoch)\n",
    "    print(f\"[CKPT] {ckpt_path}\")\n",
    "    raw = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    # --- robust state_dict extraction ---\n",
    "    def _extract_state_dict(obj):\n",
    "        # common wrappers\n",
    "        if isinstance(obj, dict):\n",
    "            for k in [\"model\", \"state_dict\", \"model_state_dict\", \"model_state\", \"net\", \"module\"]:\n",
    "                v = obj.get(k, None)\n",
    "                if isinstance(v, dict):\n",
    "                    return v\n",
    "            # maybe it's already a state_dict (tensor leaves)\n",
    "            if any(isinstance(v, torch.Tensor) for v in obj.values()):\n",
    "                return obj\n",
    "        # unknown structure; return as-is (may raise later)\n",
    "        return obj\n",
    "\n",
    "    state = _extract_state_dict(raw)\n",
    "\n",
    "    classes = read_classes(run_dir, state)\n",
    "    if state is None or (isinstance(state, dict) and len(state) == 0):\n",
    "        raise RuntimeError(\"Checkpoint does not contain a valid state_dict. Make sure training saved weights correctly.\")\n",
    "    if not classes:\n",
    "        raise RuntimeError(\"Unable to determine classes. Ensure classes.txt exists or re-train with the provided script.\")\n",
    "\n",
    "    num_classes = 1 + len(classes)\n",
    "    model = rebuild_model(num_classes, state)\n",
    "\n",
    "    # ----- prepare output folder -----\n",
    "    save_dir = os.path.join(OUTPUT_ROOT, \"saved_model\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1) state_dict\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, \"model_state_dict.pt\"))\n",
    "\n",
    "    # 2) classes files\n",
    "    with open(os.path.join(save_dir, \"classes.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in classes:\n",
    "            f.write(f\"{c}\\n\")\n",
    "    with open(os.path.join(save_dir, \"label_map.pbtxt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(make_label_map_pbtxt(classes))\n",
    "\n",
    "    # 3) config/metadata\n",
    "    meta = {\n",
    "        \"export_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"device\": DEVICE,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"classes\": classes,\n",
    "        \"source_run\": os.path.basename(run_dir),\n",
    "        \"checkpoint\": os.path.basename(ckpt_path),\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"model_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4) inference helper script\n",
    "    infer_py = os.path.join(save_dir, \"inference.py\")\n",
    "    with open(infer_py, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\"\"# Inference helper for exported PyTorch Faster R-CNN (torch 1.11 / tv 0.12)\n",
    "import os, glob, json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# pick device at runtime to avoid templating issues\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE = os.path.dirname(__file__)\n",
    "STATE_DICT = os.path.join(BASE, 'model_state_dict.pt')\n",
    "CONFIG_JSON = os.path.join(BASE, 'model_config.json')\n",
    "\n",
    "with open(CONFIG_JSON, 'r', encoding='utf-8') as f:\n",
    "    cfg = json.load(f)\n",
    "classes = cfg['classes']\n",
    "num_classes = cfg['num_classes']\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load(STATE_DICT, map_location='cpu'))\n",
    "model.eval().to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, score_thr=0.5):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = F.pil_to_tensor(img).float()/255.0\n",
    "    pred = model([x.to(DEVICE)])[0]\n",
    "    keep = pred['scores'] >= score_thr\n",
    "    return {\n",
    "        'boxes': pred['boxes'][keep].cpu().tolist(),\n",
    "        'scores': pred['scores'][keep].cpu().tolist(),\n",
    "        'labels': [classes[int(l)-1] for l in pred['labels'][keep].cpu().tolist()],\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dir = os.path.join(BASE, 'test_images')\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    print(\"Put test images in:\", test_dir)\n",
    "    for img in glob.glob(os.path.join(test_dir, '*.*')):\n",
    "        try:\n",
    "            out = predict_image(img)\n",
    "            print(os.path.basename(img), out)\n",
    "        except Exception as e:\n",
    "            print('ERR', img, e)\n",
    "\"\"\")\n",
    "\n",
    "    # 5) README\n",
    "    with open(os.path.join(save_dir, \"README.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            \"This folder mimics TensorFlow's output_inference_graph/saved_model for PyTorch.\\n\"\n",
    "            \"Always-available files: model_state_dict.pt, classes.txt, label_map.pbtxt, model_config.json, inference.py.\\n\"\n",
    "            \"Optional scripted/ONNX files are best-effort on torch 1.11 / tv 0.12.\\n\"\n",
    "        )\n",
    "\n",
    "    # ----- optional exports -----\n",
    "    example = torch.rand(3, 480, 640)  # CHW image\n",
    "    scripted = safe_torchscript(model, [example])\n",
    "    if scripted is not None:\n",
    "        scripted.save(os.path.join(save_dir, \"model_scripted.ts\"))\n",
    "        print(\"[TS] TorchScript exported -> model_scripted.ts\")\n",
    "    else:\n",
    "        print(\"[TS] TorchScript export skipped (not supported on this build)\")\n",
    "\n",
    "    onnx_path = os.path.join(save_dir, \"model.onnx\")\n",
    "    if safe_export_onnx(model, example, onnx_path):\n",
    "        print(\"[ONNX] Exported -> model.onnx\")\n",
    "    else:\n",
    "        print(\"[ONNX] Export skipped (package missing or export failed)\")\n",
    "\n",
    "    print(f\"[DONE] Exported to: {save_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954baba",
   "metadata": {},
   "source": [
    "- model_state_dict.pt : PyTorch 가중치(state_dict)\n",
    "\n",
    "- classes.txt : 한 줄당 1개 클래스\n",
    "\n",
    "- label_map.pbtxt : TF 호환 라벨맵 (툴 체인 호환용)\n",
    "\n",
    "- model_config.json : 클래스 수/이름, 소스 런/체크포인트 정보\n",
    "\n",
    "- inference.py : 이미지/폴더 추론 헬퍼 스크립트 (CLI로 바로 사용 가능)\n",
    "\n",
    "- README.txt : 사용법 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d1d29",
   "metadata": {},
   "source": [
    "11.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486b4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"\n",
    "    PyTorch 체크포인트 보관 정책:\n",
    "      - best 기준: metric이 좋아지면 저장, best.pth 갱신\n",
    "      - last 기준: 매 스텝/에폭 끝에서 최신 저장\n",
    "      - special: 특정 스텝(예: x*0.7) 통과 시 강제 저장\n",
    "      - max_to_keep: 최근 N개만 유지 (None이면 제한 없음)\n",
    "    파일명 규칙 예: ckpt_step=001234_metric=0.512.pth\n",
    "    \"\"\"\n",
    "    def __init__(self, ckpt_dir: str, max_to_keep: Optional[int] = 20):\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        os.makedirs(self.ckpt_dir, exist_ok=True)\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self._fname_re = re.compile(r\"ckpt_step=(\\d+)_metric=([0-9.]+)\\.pth\")\n",
    "\n",
    "    def _list_ckpts(self) -> List[str]:\n",
    "        return sorted([f for f in os.listdir(self.ckpt_dir) if f.endswith(\".pth\") and f.startswith(\"ckpt_step=\")])\n",
    "\n",
    "    def _prune(self):\n",
    "        if self.max_to_keep is None:\n",
    "            return\n",
    "        files = self._list_ckpts()\n",
    "        excess = len(files) - self.max_to_keep\n",
    "        for i in range(excess):\n",
    "            try:\n",
    "                os.remove(os.path.join(self.ckpt_dir, files[i]))\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    def save(self, step: int, metric: float, state: Dict[str, Any], tag: Optional[str] = None):\n",
    "        fname = f\"ckpt_step={step:06d}_metric={metric:.6f}.pth\"\n",
    "        fpath = os.path.join(self.ckpt_dir, fname)\n",
    "        torch_save = state.pop(\"_torch_save\", None)  # (희소 확장 포인트)\n",
    "        if torch_save is not None:\n",
    "            # 커스텀 저장 함수가 오면 사용\n",
    "            torch_save(fpath, state)\n",
    "        else:\n",
    "            import torch\n",
    "            torch.save(state, fpath)\n",
    "\n",
    "        # last.pth 복사\n",
    "        shutil.copy2(fpath, os.path.join(self.ckpt_dir, \"last.pth\"))\n",
    "\n",
    "        # best 태그가 오면 best.pth 갱신\n",
    "        if tag == \"best\":\n",
    "            shutil.copy2(fpath, os.path.join(self.ckpt_dir, \"best.pth\"))\n",
    "\n",
    "        # 개수 제한\n",
    "        self._prune()\n",
    "        return fpath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b51ec0d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.checkpoint_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     18\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# 프로젝트 루트로 조정\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckpointManager\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 0) 하이퍼파라미터/설정\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTrainConfig\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.checkpoint_manager'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PyTorch 1.11 훈련 스켈레톤:\n",
    " - TF의 x*0.7 시점 강제 저장\n",
    " - TensorBoard 로깅\n",
    " - best/last 체크포인트 관리\n",
    " - 하이퍼파라미터 덤프\n",
    "\"\"\"\n",
    "import os, json, time, yaml, math, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))  # 프로젝트 루트로 조정\n",
    "\n",
    "from utils.checkpoint_manager import CheckpointManager\n",
    "\n",
    "# -----------------------\n",
    "# 0) 하이퍼파라미터/설정\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    output_root: str = r\"D:\\AI_SVT_Training_mk\\output_inference_graph_pytorch\"\n",
    "    max_steps: int = 10000             # TF에서 쓰시던 X\n",
    "    batch_size: int = 8\n",
    "    base_lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    metric_name: str = \"val/mAP\"       # 베스트 판별 지표 이름\n",
    "    save_fraction: float = 0.7         # x*0.7 지점\n",
    "    ckpt_max_to_keep: int = 20         # ★ 여기 때문에 \"20개만\" 남습니다. 바꾸세요.\n",
    "    log_every: int = 50\n",
    "    eval_every: int = 200              # 간단 검증 주기(데모)\n",
    "    model_name: str = \"resnet50_frcnn\" # 예시 표기용\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# -----------------------\n",
    "# 1) 예시 모델 (분류기로 데모)\n",
    "#    → 실제 FRCNN/검출모델로 교체하세요.\n",
    "# -----------------------\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, in_dim=1024, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------\n",
    "# 2) 데이터 로더 (데모)\n",
    "# -----------------------\n",
    "def get_fake_batch(bs, in_dim=1024, num_classes=4, device=\"cpu\"):\n",
    "    x = torch.randn(bs, in_dim, device=device)\n",
    "    y = torch.randint(0, num_classes, (bs,), device=device)\n",
    "    return x, y\n",
    "\n",
    "# -----------------------\n",
    "# 3) 훈련 루프\n",
    "# -----------------------\n",
    "def main():\n",
    "    cfg = TrainConfig()\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    # 디렉토리\n",
    "    ckpt_dir = os.path.join(cfg.output_root, \"checkpoints\")\n",
    "    log_dir_train = os.path.join(cfg.output_root, \"logs\", \"train\")\n",
    "    log_dir_val   = os.path.join(cfg.output_root, \"logs\", \"val\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir_train, exist_ok=True)\n",
    "    os.makedirs(log_dir_val, exist_ok=True)\n",
    "\n",
    "    # 하이퍼파라미터 스냅샷\n",
    "    with open(os.path.join(cfg.output_root, \"saved_model\", \"config.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        os.makedirs(os.path.join(cfg.output_root, \"saved_model\"), exist_ok=True)\n",
    "        yaml.safe_dump(asdict(cfg), f, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "    writer_tr = SummaryWriter(log_dir_train)\n",
    "    writer_va = SummaryWriter(log_dir_val)\n",
    "\n",
    "    # 모델/손실/옵티마이저\n",
    "    model = TinyNet(in_dim=1024, num_classes=4).to(cfg.device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.base_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # 그래프 등록(선택)\n",
    "    try:\n",
    "        dummy = torch.randn(1, 1024).to(cfg.device)\n",
    "        writer_tr.add_graph(model, dummy)\n",
    "    except Exception:\n",
    "        pass  # 일부 환경에서 add_graph가 실패할 수 있음\n",
    "\n",
    "    # 체크포인트 관리자\n",
    "    ckptm = CheckpointManager(ckpt_dir, max_to_keep=cfg.ckpt_max_to_keep)\n",
    "\n",
    "    # 진행 변수\n",
    "    global_step = 0\n",
    "    best_metric = -1e9\n",
    "    save_at = int(cfg.max_steps * cfg.save_fraction)  # ★ x*0.7 지점\n",
    "\n",
    "    while global_step < cfg.max_steps:\n",
    "        model.train()\n",
    "        x, y = get_fake_batch(cfg.batch_size, device=cfg.device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        # 로그\n",
    "        if global_step % cfg.log_every == 0:\n",
    "            writer_tr.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "\n",
    "        # 간단 검증 (데모)\n",
    "        if global_step % cfg.eval_every == 0 or global_step == cfg.max_steps:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # 실제 프로젝트에서는 mAP/IoU 등의 검증 코드를 넣으세요.\n",
    "                # 여기선 데모로 \"val/mAP\"를 (무작위에 노이즈 더한) 증가형 수치로 가정\n",
    "                fake_val_map = 0.2 + 0.8 * (global_step / cfg.max_steps) + (torch.randn(1).item() * 0.01)\n",
    "                writer_va.add_scalar(cfg.metric_name, fake_val_map, global_step)\n",
    "\n",
    "                # 베스트 갱신 시 저장\n",
    "                if fake_val_map > best_metric:\n",
    "                    best_metric = fake_val_map\n",
    "                    ckptm.save(\n",
    "                        step=global_step,\n",
    "                        metric=fake_val_map,\n",
    "                        state={\n",
    "                            \"step\": global_step,\n",
    "                            \"model\": model.state_dict(),\n",
    "                            \"optimizer\": optimizer.state_dict(),\n",
    "                            \"best_metric\": best_metric,\n",
    "                            \"config\": asdict(cfg),\n",
    "                        },\n",
    "                        tag=\"best\"\n",
    "                    )\n",
    "\n",
    "        # x*0.7 지점 첫 통과 시 강제 저장\n",
    "        if global_step == save_at:\n",
    "            ckptm.save(\n",
    "                step=global_step,\n",
    "                metric=best_metric if best_metric > -1e9 else 0.0,\n",
    "                state={\n",
    "                    \"step\": global_step,\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"best_metric\": best_metric,\n",
    "                    \"config\": asdict(cfg),\n",
    "                },\n",
    "                tag=None\n",
    "            )\n",
    "\n",
    "    # 마지막 스냅샷\n",
    "    ckptm.save(\n",
    "        step=global_step,\n",
    "        metric=best_metric if best_metric > -1e9 else 0.0,\n",
    "        state={\n",
    "            \"step\": global_step,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "            \"config\": asdict(cfg),\n",
    "        },\n",
    "        tag=None\n",
    "    )\n",
    "\n",
    "    writer_tr.close(); writer_va.close()\n",
    "    print(f\"[DONE] Training finished at step={global_step}, best={best_metric:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7aab1e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "best.pth 가 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 103\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[EXPORTED] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → saved_model/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# 예시) 1) best로 추출\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# export_from_ckpt(use_best=True)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# 예시) 2) TensorBoard 보고 '6000스텝'이 좋았다면:\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# export_from_ckpt(step=6000, use_best=False)\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[43mexport_from_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mexport_from_ckpt\u001b[1;34m(step, use_best, onnx, torchscript)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_best:\n\u001b[0;32m     44\u001b[0m     ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CKPT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(ckpt_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pth 가 없습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: best.pth 가 없습니다."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "원하는 스텝의 체크포인트(.pth)를 로드하여\n",
    " - TorchScript (script or trace)\n",
    " - ONNX\n",
    "를 D:\\AI_SVT_Training_mk\\output_inference_graph_pytorch\\saved_model\\ 에 생성\n",
    "\"\"\"\n",
    "import os, json, time, glob, re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "SAVE_ROOT = r\"D:\\AI_SVT_Training_mk\\output_inference_graph_pytorch\"\n",
    "CKPT_DIR  = os.path.join(SAVE_ROOT, \"checkpoints\")\n",
    "OUT_DIR   = os.path.join(SAVE_ROOT, \"saved_model\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 학습 때와 동일한 모델 클래스를 가져와야 합니다.\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, in_dim=1024, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def find_ckpt_by_step(step: int):\n",
    "    pat = re.compile(rf\"ckpt_step={step:06d}_metric=([0-9.]+)\\.pth\")\n",
    "    for f in os.listdir(CKPT_DIR):\n",
    "        m = pat.match(f)\n",
    "        if m:\n",
    "            return os.path.join(CKPT_DIR, f), float(m.group(1))\n",
    "    return None, None\n",
    "\n",
    "def load_state(ckpt_path: str):\n",
    "    obj = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    return obj\n",
    "\n",
    "def export_from_ckpt(step: int = None, use_best: bool = False, onnx: bool = True, torchscript: bool = True):\n",
    "    if use_best:\n",
    "        ckpt_path = os.path.join(CKPT_DIR, \"best.pth\")\n",
    "        assert os.path.isfile(ckpt_path), \"best.pth 가 없습니다.\"\n",
    "        metric = None\n",
    "    else:\n",
    "        assert step is not None, \"step을 지정하거나 use_best=True 하세요.\"\n",
    "        ckpt_path, metric = find_ckpt_by_step(step)\n",
    "        assert ckpt_path is not None, f\"해당 스텝({step}) ckpt가 없습니다.\"\n",
    "\n",
    "    state = load_state(ckpt_path)\n",
    "    cfg = state.get(\"config\", {})\n",
    "    num_classes = 4  # 실제 프로젝트에 맞추세요\n",
    "    model = TinyNet(in_dim=1024, num_classes=num_classes)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    # 더미 입력 (실제 입력 형태로 수정)\n",
    "    dummy = torch.randn(1, 1024)\n",
    "\n",
    "    # 1) TorchScript\n",
    "    if torchscript:\n",
    "        try:\n",
    "            scripted = torch.jit.script(model)\n",
    "        except Exception:\n",
    "            scripted = torch.jit.trace(model, dummy)\n",
    "        ts_path = os.path.join(OUT_DIR, \"model_ts.pt\")\n",
    "        scripted.save(ts_path)\n",
    "\n",
    "    # 2) ONNX\n",
    "    if onnx:\n",
    "        onnx_path = os.path.join(OUT_DIR, \"model.onnx\")\n",
    "        torch.onnx.export(\n",
    "            model, dummy, onnx_path,\n",
    "            input_names=[\"input\"], output_names=[\"logits\"],\n",
    "            opset_version=13, do_constant_folding=True, verbose=False\n",
    "        )\n",
    "\n",
    "    # 3) 메타 기록\n",
    "    meta = {\n",
    "        \"export_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"by\": \"export.py\",\n",
    "        \"step\": int(state.get(\"step\", -1)),\n",
    "        \"metric_name\": cfg.get(\"metric_name\", \"val/mAP\"),\n",
    "        \"best_metric\": float(state.get(\"best_metric\", float(\"nan\"))),\n",
    "        \"source_ckpt\": os.path.basename(ckpt_path),\n",
    "        \"onnx\": bool(onnx),\n",
    "        \"torchscript\": bool(torchscript)\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[EXPORTED] {ckpt_path} → saved_model/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 예시) 1) best로 추출\n",
    "    # export_from_ckpt(use_best=True)\n",
    "\n",
    "    # 예시) 2) TensorBoard 보고 '6000스텝'이 좋았다면:\n",
    "    # export_from_ckpt(step=6000, use_best=False)\n",
    "\n",
    "    export_from_ckpt(use_best=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.checkpoint_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint_manager\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.checkpoint_manager'"
     ]
    }
   ],
   "source": [
    "import utils.checkpoint_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62581dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "403eb593",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107df51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59b081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbf19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31154bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607831a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56805da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
